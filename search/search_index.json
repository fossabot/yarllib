{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"yarllib Yet Another Reinforcement Learning Library. Status: development . Why? I had the need for a RL library/framework that: - was clearly and simply implemented, with good enough performances; - highly focused on modularity, customizability and extendability; - wasn't merely Deep Reinforcement Learning oriented. I couldn't find an existing library that satisfied my needs; hence I decided to implement yet another RL library. For me it is also an opportunity to have a better understanding of the RL algorithms and to appreciate the nuances that you can't find on a book. If you find this repo useful for your research or your project, I'd be very glad :-) don't hesitate to reach me out! What The package is both: - a library , because it provides off-the-shelf functionalities to set up an RL experiment; - a framework , because you can compose your custom model by implementing the interfaces, override the default behaviours, or use the existing components as-is. You can find more details in the documentation . Tests To run tests: tox To run only the code tests: tox -e py3.7 To run only the linters: - tox -e flake8 - tox -e mypy - tox -e black-check - tox -e isort-check Please look at the tox.ini file for the full list of supported commands. Docs To build the docs: mkdocs build To view documentation in a browser: mkdocs serve and then go to http://localhost:8000 License yarllib is released under the GNU Lesser General Public License v3.0 or later (LGPLv3+). Copyright 2020 Marco Favorito Authors Marco Favorito","title":"Home"},{"location":"#why","text":"I had the need for a RL library/framework that: - was clearly and simply implemented, with good enough performances; - highly focused on modularity, customizability and extendability; - wasn't merely Deep Reinforcement Learning oriented. I couldn't find an existing library that satisfied my needs; hence I decided to implement yet another RL library. For me it is also an opportunity to have a better understanding of the RL algorithms and to appreciate the nuances that you can't find on a book. If you find this repo useful for your research or your project, I'd be very glad :-) don't hesitate to reach me out!","title":"Why?"},{"location":"#what","text":"The package is both: - a library , because it provides off-the-shelf functionalities to set up an RL experiment; - a framework , because you can compose your custom model by implementing the interfaces, override the default behaviours, or use the existing components as-is. You can find more details in the documentation .","title":"What"},{"location":"#tests","text":"To run tests: tox To run only the code tests: tox -e py3.7 To run only the linters: - tox -e flake8 - tox -e mypy - tox -e black-check - tox -e isort-check Please look at the tox.ini file for the full list of supported commands.","title":"Tests"},{"location":"#docs","text":"To build the docs: mkdocs build To view documentation in a browser: mkdocs serve and then go to http://localhost:8000","title":"Docs"},{"location":"#license","text":"yarllib is released under the GNU Lesser General Public License v3.0 or later (LGPLv3+). Copyright 2020 Marco Favorito","title":"License"},{"location":"#authors","text":"Marco Favorito","title":"Authors"},{"location":"authors/","text":"Credits Maintainers Marco Favorito < favorito@diag.uniroma1.it > Contributors None yet. Why not be the first ?","title":"Authors"},{"location":"authors/#credits","text":"","title":"Credits"},{"location":"authors/#maintainers","text":"Marco Favorito < favorito@diag.uniroma1.it >","title":"Maintainers"},{"location":"authors/#contributors","text":"None yet. Why not be the first ?","title":"Contributors"},{"location":"contributing/","text":"Contributing Contributions are welcome, and greatly appreciated! Every little bit helps, and credit will always be given. If you need support, want to report/fix a bug, ask for/implement features, you can check the Issues page or submit a Pull request . For other kinds of feedback, you can contact one of the authors by email.","title":"How to contribute"},{"location":"contributing/#contributing","text":"Contributions are welcome, and greatly appreciated! Every little bit helps, and credit will always be given. If you need support, want to report/fix a bug, ask for/implement features, you can check the Issues page or submit a Pull request . For other kinds of feedback, you can contact one of the authors by email.","title":"Contributing"},{"location":"quickstart/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Quickstart Example: The Cliff Walking Let's solve the famous Cliff Walking problem with tabular Q-Learning and SARSA [1] . Define the environment: from gym.envs.toy_text import CliffWalkingEnv import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from pandas import DataFrame from yarllib.helpers.history import History from yarllib.policies import EpsGreedyPolicy env = CliffWalkingEnv () def print_summary ( history : History ): print ( \"Training statistics:\" ) print ( f \"Number of episodes: { history . nb_episodes } \" ) print ( f \"Average total reward: { history . total_rewards . mean () } \" ) print ( f \"Average number of steps: { history . lengths . mean () } \" ) print ( f \"Average total reward (last 50 episodes): { history . total_rewards [: - 50 ] . mean () } \" ) print ( f \"Average number of steps (last 50 episodes): { history . lengths [: - 50 ] . mean () } \" ) First, let's define the parameters, common both to Q-Learning and SARSA: nb_steps = 30000 alpha = 0.1 gamma = 0.99 seed = 42 epsilon = 0.1 policy = EpsGreedyPolicy ( epsilon ) params = dict ( env = env , nb_steps = nb_steps , policy = policy , alpha = alpha , gamma = gamma , seed = seed ) Define the Q-Learning agent: from yarllib.models.tabular import TabularQLearning qlearning = TabularQLearning ( env . observation_space , env . action_space ) . agent () print ( f \"Table dimensions: { qlearning . model . q . shape } \" ) Table dimensions: (48, 4) \u0002hzzhzkh:8\u0003 Run for 30000 steps using \\varepsilon \\varepsilon -greedy policy with \\varepsilon = 0.1 \\varepsilon = 0.1 : qlearning_history = qlearning . train ( ** params ) print_summary ( qlearning_history ) Training statistics: Number of episodes: 1273 Average total reward: -57.92537313432836 Average number of steps: 23.551453260015712 Average total reward (last 50 episodes): -58.831561733442356 Average number of steps (last 50 episodes): 23.86181520850368 \u0002hzzhzkh:12\u0003 Define and train a SARSA agent: from yarllib.models.tabular import TabularSarsa sarsa = TabularSarsa ( env . observation_space , env . action_space ) . agent () sarsa_history = sarsa . train ( ** params ) print_summary ( sarsa_history ) Training statistics: Number of episodes: 1222 Average total reward: -33.770049099836335 Average number of steps: 24.534369885433716 Average total reward (last 50 episodes): -34.37542662116041 Average number of steps (last 50 episodes): 24.830204778156997 \u0002hzzhzkh:16\u0003 Compare the sum of rewards: def _plot ( histories , labels ): assert len ( histories ) == len ( labels ) for h , l in zip ( histories , labels ): data = h . total_rewards df = DataFrame ( data . T ) df = pd . concat ([ df [ col ] for col in df ]) df = df . rolling ( 200 ) . mean () sns . lineplot ( data = df , label = l ) plt . xlim ( 175 , 600 ) _plot ([ qlearning_history , sarsa_history ], [ \"q-learning\" , \"sarsa\" ]) \u0002hzzhzkh:20\u0003 We get what we expected: Q-Learning performs worse than SARSA, as explained in the Example 6.6 in the Sutton & Barto textbook [1] .","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#example-the-cliff-walking","text":"Let's solve the famous Cliff Walking problem with tabular Q-Learning and SARSA [1] . Define the environment: from gym.envs.toy_text import CliffWalkingEnv import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from pandas import DataFrame from yarllib.helpers.history import History from yarllib.policies import EpsGreedyPolicy env = CliffWalkingEnv () def print_summary ( history : History ): print ( \"Training statistics:\" ) print ( f \"Number of episodes: { history . nb_episodes } \" ) print ( f \"Average total reward: { history . total_rewards . mean () } \" ) print ( f \"Average number of steps: { history . lengths . mean () } \" ) print ( f \"Average total reward (last 50 episodes): { history . total_rewards [: - 50 ] . mean () } \" ) print ( f \"Average number of steps (last 50 episodes): { history . lengths [: - 50 ] . mean () } \" ) First, let's define the parameters, common both to Q-Learning and SARSA: nb_steps = 30000 alpha = 0.1 gamma = 0.99 seed = 42 epsilon = 0.1 policy = EpsGreedyPolicy ( epsilon ) params = dict ( env = env , nb_steps = nb_steps , policy = policy , alpha = alpha , gamma = gamma , seed = seed ) Define the Q-Learning agent: from yarllib.models.tabular import TabularQLearning qlearning = TabularQLearning ( env . observation_space , env . action_space ) . agent () print ( f \"Table dimensions: { qlearning . model . q . shape } \" ) Table dimensions: (48, 4) \u0002hzzhzkh:8\u0003 Run for 30000 steps using \\varepsilon \\varepsilon -greedy policy with \\varepsilon = 0.1 \\varepsilon = 0.1 : qlearning_history = qlearning . train ( ** params ) print_summary ( qlearning_history ) Training statistics: Number of episodes: 1273 Average total reward: -57.92537313432836 Average number of steps: 23.551453260015712 Average total reward (last 50 episodes): -58.831561733442356 Average number of steps (last 50 episodes): 23.86181520850368 \u0002hzzhzkh:12\u0003 Define and train a SARSA agent: from yarllib.models.tabular import TabularSarsa sarsa = TabularSarsa ( env . observation_space , env . action_space ) . agent () sarsa_history = sarsa . train ( ** params ) print_summary ( sarsa_history ) Training statistics: Number of episodes: 1222 Average total reward: -33.770049099836335 Average number of steps: 24.534369885433716 Average total reward (last 50 episodes): -34.37542662116041 Average number of steps (last 50 episodes): 24.830204778156997 \u0002hzzhzkh:16\u0003 Compare the sum of rewards: def _plot ( histories , labels ): assert len ( histories ) == len ( labels ) for h , l in zip ( histories , labels ): data = h . total_rewards df = DataFrame ( data . T ) df = pd . concat ([ df [ col ] for col in df ]) df = df . rolling ( 200 ) . mean () sns . lineplot ( data = df , label = l ) plt . xlim ( 175 , 600 ) _plot ([ qlearning_history , sarsa_history ], [ \"q-learning\" , \"sarsa\" ]) \u0002hzzhzkh:20\u0003 We get what we expected: Q-Learning performs worse than SARSA, as explained in the Example 6.6 in the Sutton & Barto textbook [1] .","title":"Example: The Cliff Walking"},{"location":"references/","text":"References Follows the list of references used across the documentation. 1 : Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249.","title":"References"},{"location":"references/#references","text":"Follows the list of references used across the documentation. 1 : Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249.","title":"References"}]}